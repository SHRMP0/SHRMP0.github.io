---
title: "Crime pays for big tech"
date: 2025-08-11
# weight: 1
# slug: big-tech-child-exploitation
# aliases: ["/en/posts/destroy-big-tech"]
tags: ["big tech"]
author: ["BIANA", "SHRMP0 (translation)"] # ["Me", "You"] multiple authors
showToc: true
TocOpen: false
draft: true
hidemeta: false
comments: false
bsky: "<bsky post url>" # link to your bsky post
description: "Every child exploited is profit."
disableShare: true
disableHLJS: false # to disable highlightjs
hideSummary: false
searchHidden: false
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: true
ShowRssButtonInSectionTermList: true
UseHugoToc: true
# images: ["/en/posts/big-tech-child-exploitation/images/felca-adultizacao.en.jpg"] # link or path of image for opengraph, twitter-cards
cover:
    image: "/en/posts/big-tech-child-exploitation/images/felca-adultizacao.en.jpg" # image path/url
    alt: "Drawing of a thumbnail from Felca's YouTube channel." # alt text
    caption: "Drawing of a thumbnail from Felca's YouTube channel. Art: [magdiel](https://bsky.app/profile/137magdiel.bsky.social/post/3lw527kid3s2n)" # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: false # only hide on current single page
editPost:
    URL: "https://github.com/SHRMP0/SHRMP0.github.io/tree/main/content"
    Text: "Suggest changes" # edit text
    appendFilePath: true # to append file path to Edit link
---

**Disclaimer**: this text was written by [BIANA](https://bsky.app/profile/bianaaf.bsky.social), a brazilian independent artist, communist, designer and researcher at the Ethnographic Laboratory of Technological and Digital Studies (LETEC-USP). I (SHRMP0) was only responsible for translating it to english. *The original post (in portuguese) is available [here](https://bianaaf.medium.com/o-crime-compensa-para-as-big-techs-cada-crian%C3%A7a-explorada-%C3%A9-lucro-d459bf326fd5).*

---

Quando o Felca publicou seu vídeo sobre “adultização”, muitos o viram como mais um alerta aos pais sobre os riscos da internet. De fato era, mas, na prática, o que ele fez foi um experimento empírico que escancara o funcionamento de uma das engrenagens mais lucrativas e sombrias das redes sociais.

A ideia parecia simples: criar uma conta do zero no Instagram. Nenhum histórico, nenhum seguidor, nenhuma postagem anterior. Como qualquer usuário novo, o que determinaria o que apareceria no feed seriam as interações a partir dali e, desde o primeiro clique, Felca decidiu interagir apenas com conteúdos de crianças, com curtidas e salvamentos, direcionados exclusivamente a perfis com conteúdos infantis. Não havia nada de aleatório, era um “treinamento” do algoritmo, reproduzindo o que uma pessoa com interesses criminosos poderia fazer para encontrar e consumir esse tipo de conteúdo.

O que aconteceu depois não levou dias ou semanas: em poucas horas, as recomendações começaram a incluir conteúdos sexualizados de crianças, chegando a material que configura pornografia infantil. Os conteúdos, como ele explica no vídeos, iam desde conteúdos sugestivos, até material explícito circulando dentro da plataforma. A velocidade com que isso aconteceu revela algo fundamental: **o algoritmo não “errou”. Ele fez exatamente o que foi projetado para fazer** — detectar um padrão de interesse e reforçá-lo com mais do mesmo, **sem qualquer filtro moral ou legal**, desde que isso aumente o tempo de permanência do usuário.

Felca menciona a responsabilidade do Instagram, mas seu foco era outro: alertar os pais para que não exponham seus filhos online. O recado é válido: qualquer foto ou vídeo postado pode ser capturado, redirecionado e entrar num ecossistema que lucra com a exploração. Mas, parar nesse alerta, é olhar apenas para a superfície. O experimento evidencia que, embora pais e consumidores destes conteúdos criminosos tenham responsabilidade na cadeia de exploração, o núcleo que sustenta e potencializa essa engrenagem está nas plataformas que a projeta, opera e lucra diretamente com seu funcionamento.

## O algoritmo como cúmplice estrutural

O Instagram, como todas as grandes redes, opera com sistemas de recomendação baseados em aprendizado de máquina. Esses sistemas analisam cada clique, cada curtida, cada segundo de vídeo assistido, cada hashtag pesquisada. Com isso, criam um perfil dinâmico do usuário e tentam prever o que ele mais provavelmente vai querer ver em seguida. Essas previsões não são para “melhorar a experiência” de forma altruísta, mas, são o motor da monetização. Quanto mais preciso o sistema for em manter você rolando a tela, mais anúncios podem ser exibidos e mais dados podem ser coletados para vender publicidade segmentada.

E aqui está o ponto crítico: **não há distinção moral na métrica**. Um vídeo educativo e um vídeo criminoso são igualmente valiosos se ambos mantêm o usuário preso à plataforma. Como revelaram os *Facebook Papers*, conteúdos que despertam emoções intensas — raiva, choque, desejo — tendem a performar melhor em termos de engajamento e, portanto, são priorizados.

No caso da exploração infantil, há ainda um agravante perverso: **o sistema algorítmico não diferencia o tipo de atenção recebida**. Interações feitas com intenção criminosa e interações motivadas por denúncia ou repúdio são processadas como sinais de interesse, o que pode levar a um aumento na recomendação do conteúdo, independente do impacto geral que ele causa.

O que Felca documentou se encaixa em um padrão conhecido. Como demonstrou investigação conjunta do *Wall Street Journal*, da Universidade de Stanford e da Universidade de Massachusetts, seguir apenas algumas recomendações relacionadas a hashtags usadas por pedófilos foi suficiente para que uma conta de teste fosse inundada com conteúdo que sexualiza crianças, incluindo material ilegal, e conectada a redes de compra e venda desse conteúdo. Em 2024, pesquisadores dinamarqueses publicaram 85 conteúdos de automutilação em uma rede privada no Instagram; nenhum foi removido após um mês, apesar de a Meta afirmar que remove 99% desse tipo de material rapidamente. Em outro caso, um vídeo de decapitação permaneceu no YouTube por cerca de cinco horas antes de ser retirado, acumulando milhares de visualizações e exibindo anúncios. Pesquisas publicadas no *Washington Post* em 2025 indicam que 90% de todas as interações com um post ocorrem nas primeiras 30 horas, de modo que remoções tardias têm pouco efeito no alcance.

Desde seus primeiros anos, as big techs tratam a moderação como um custo, não como prioridade. A escala gigantesca de publicações diárias serve como escudo: sempre é possível alegar que “é impossível monitorar tudo”. Essa é a política da *plausible deniability* — processos deliberadamente opacos, que permitem à empresa afirmar que não sabia, ou que “agiu assim que foi informada”, mesmo quando evidências mostram o contrário.

Esse quadro se agrava em países latino-americanos e do Sul Global. Aqui, há menos investimento em equipes de moderação que falam as línguas locais ou compreendem o contexto cultural. Segundo dados revelados por Frances Haugen em audiência no Senado dos EUA, **87% dos recursos do Facebook destinados ao combate à desinformação eram voltados para conteúdo em inglês, embora apenas cerca de 9% dos usuários da plataforma falem esse idioma**. Essa disparidade reflete um padrão mais amplo: até 2018, o Facebook não possuía sistemas automáticos para identificar discurso de ódio em hindi e bengali, e, em 2020, idiomas como amárico e oromo ainda não contavam com classificadores para moderação. No Brasil, durante as eleições de 2018 e 2022, estudos do NetLab/UFRJ documentaram como o YouTube e o Facebook favoreceram, via recomendação, canais e páginas que difundiam teorias conspiratórias e desinformação eleitoral. Mesmo após denúncias formais ao TSE, muitos desses conteúdos permaneceram no ar até depois da votação.

A lógica é simples e estarrecedora: remover custa caro; manter por mais algumas horas é barato e lucrativo. Além disso, todos estes dados reforçam o argumento que mobilizei no meu texto [“1 esquerdista contra 20 big techs conservadoras”](https://bianaaf.medium.com/1-esquerdista-contra-20-big-techs-conservadoras-contribui%C3%A7%C3%A3o-para-a-formula%C3%A7%C3%A3o-de-uma-postura-e45f54593362) de que, **o problema das big techs não é apenas moderação deficiente, mas um problema de modelo de negócio**, que envolve toda a arquitetura (desde o design de engajamento até o modelo de recomendação).

Esse é o ponto em que a crítica precisa ir além da moderação. **O problema central das redes sociais, ao colocarem o lucro acima de qualquer outro valor, é que essa lógica as torna funcionalmente fascistas**. Todas as empresas sob o capitalismo operam sob a premissa da maximização de lucro, mas as big techs detêm um tipo específico de capital — o controle concentrado de dados, da infraestrutura de comunicação e da capacidade de moldar comportamentos em escala global — que lhes confere um poder político e social inédito. Quando esse poder é orientado para sustentar um modelo que tolera e até amplifica violência, desigualdade e exploração, ele deixa de ser apenas uma questão de mercado: torna-se um mecanismo ativo de manutenção de estruturas autoritárias.

## Precarização digital e controle político

No Sul Global, essa negligência é estrutural. Enquanto EUA e UE contam com regulações mais rígidas, nossos países ficam com moderação mais lenta, menos recursos e menos transparência. **Isso cria terreno fértil para a exploração não apenas sexual, mas também política e informacional**.

O efeito político é direto: a mesma arquitetura que facilita a circulação de pornografia infantil, facilita também a disseminação de propaganda autoritária. O algoritmo não vê diferença. Para ele, tudo é engajamento.

A estrutura das grandes plataformas digitais abriga um conflito de interesses central: de um lado, a sociedade demanda justiça social e emancipação humana, o que inclui a proteção integral de crianças e a erradicação da exploração infantil; de outro, o modelo de negócios das big techs é baseado na maximização de engajamento e extração de dados, mesmo quando isso significa expor, amplificar ou monetizar conteúdo que agride esses mesmos princípios.

Essa contradição não é acidental: ela cria um ambiente em que interesses econômicos e políticos se alinham contra a proteção de grupos vulneráveis. Quando esses interesses se articulam com narrativas autoritárias, de controle social e naturalização da violência, o espaço resultante não é neutro — é um espaço **estruturalmente fascista**. O fascismo, aqui, não é apenas uma ideologia explícita, mas uma lógica operacional: a concentração de poder e decisão em atores privados, a desumanização como ferramenta de controle e a manutenção deliberada de desigualdades que beneficiam os que estão no topo.

Nesse sentido, o combate à exploração infantil não é só uma pauta humanitária isolada — é parte de um projeto mais amplo de emancipação humana que se choca diretamente com os interesses estruturais das plataformas. Ao permitir que crimes sejam explorados como fontes de dados e engajamento, as big techs **não apenas falham na proteção: elas participam ativamente da manutenção de uma ordem que é, em sua lógica, hostil à justiça social**.

Por isso, dizer que “as redes sociais de massa são apenas o meio” é ingenuidade. **Elas são ferramentas políticas do fascismo**. A mesma lógica que levou o Instagram a recomendar pornografia infantil a um perfil “treinado” para consumir conteúdo de crianças é a que recomenda discurso de ódio e teorias conspiratórias a perfis interessados em política extremista.

## Responsabilidade individual e responsabilidade estrutural

No debate sobre exploração infantil online, é preciso reconhecer que existem dois planos distintos de responsabilidade. No plano individual, estão pais e responsáveis que expõem crianças, e consumidores finais que alimentam a demanda por esse material. No plano estrutural, estão as plataformas que projetam, operam e lucram com a arquitetura que permite, amplifica e monetiza esses conteúdos.

A responsabilidade individual é moral e social: diz respeito às escolhas e ações concretas de cada ator. Já a responsabilidade estrutural é sistêmica: envolve o controle dos algoritmos, a posse e uso de dados, e a definição das regras que moldam toda a experiência digital. **As plataformas têm poder técnico e econômico para interromper a lógica de funcionamento que alimenta o problema — mas não o fazem, porque isso implicaria reduzir lucro**.

Enquanto a exploração for rentável, ela persistirá. E enquanto as plataformas mantiverem esse poder estrutural sobre o ecossistema, sua responsabilidade não poderá ser dissociada do próprio crime.

{{< figure align="center" src="/en/posts/big-tech-child-exploitation/images/destruir-big-techs.en.jpg" alt="Godzilla vs Mecha Godzilla drawing with \"Destroy big tech before they destroy us\" written on it" caption="Translation: \"Destroy big tech before they destroy us\". Art: [magdiel](https://bsky.app/profile/137magdiel.bsky.social/post/3lw52hrf3422n)" >}}

---

**Fontes**:

**Cathy O’Neil**. *Weapons of Math Destruction*.  
**Max Fisher**. *The Chaos Machine*.  
**Shoshana Zuboff**. *The Age of Surveillance Capitalism*.  
**Letícia Cesarino**. *O mundo do avesso: verdade e política na era digital*.  
**Leandro Konder**. *Introdução ao fascismo*.  
**Jean Burgess; Nancy K. Baym**. *Twitter: A Biography*.  
**BBC**. [*Conspiração e apuração paralela: a desinformação sobre urnas que circula no WhatsApp e Telegram às vésperas da eleição*](https://www.bbc.com/portuguese/brasil-63097867).  
**The Wall Street Journal**. [*Facebook tried to make its platform a healthier place. It got angrier instead*](https://www.wsj.com/tech/facebook-algorithm-change-zuckerberg-11631654215).  
**The Guardian**. [*Instagram ‘actively helping’ spread of self-harm among teenagers, study suggests*](https://www.theguardian.com/technology/2024/nov/30/instagram-actively-helping-to-spread-of-self-harm-among-teenagers-study-suggests).  
**APNews**. [*A beheading video was on YouTube for hours, raising questions about why it wasn’t taken down sooner*](https://apnews.com/article/beheaded-father-pennsylvania-man-youtube-2d9231aa710324d07b1729f02b3d6a61).  
**Washington Post**. [*Researchers call for new way of thinking about content moderation*](https://www.washingtonpost.com/politics/2025/04/29/facebook-content-moderation-delays/).  
**TechCrunch**. [*Facebook whistleblower Frances Haugen testifies before the Senate*](https://techcrunch.com/2021/10/05/facebook-whistleblower-frances-haugen-testifies-before-the-senate/).  
**Rest of World**. [*Stat of the day*](https://restofworld.org/stat-of-the-day/haugen-facebook-moderation/).  
**Rest of World**. [*The Facebook Papers reveal staggering failures in the Global South*](https://restofworld.org/2021/facebook-papers-reveal-staggering-failures-in-global-south/).  
**The Wall Street Journal**. [*Instagram connects vast pedophile network*](https://www.wsj.com/tech/instagram-vast-pedophile-network-4ab7189).
